{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Create a neural network",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bacdam91/mxnet-tutorial/blob/master/Create_a_neural_network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oKVygAfT-6Vx",
        "colab_type": "code",
        "outputId": "aefdbc34-dacc-4487-ffd6-bfd62422efa3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "!pip install mxnet"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: mxnet in /usr/local/lib/python3.6/dist-packages (1.5.1.post0)\n",
            "Requirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.6/dist-packages (from mxnet) (2.21.0)\n",
            "Requirement already satisfied: graphviz<0.9.0,>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from mxnet) (0.8.4)\n",
            "Requirement already satisfied: numpy<2.0.0,>1.16.0 in /usr/local/lib/python3.6/dist-packages (from mxnet) (1.17.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet) (2019.11.28)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet) (3.0.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ga9E6HQMUCze",
        "colab_type": "text"
      },
      "source": [
        "### Create a neural network\n",
        "\n",
        "Creating a neural network with ```MXNet``` is quite simple. In this tutorial, we will be going through the basic of setting up a neural net in ```MXNet```."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7DtZ0P3UWBY",
        "colab_type": "text"
      },
      "source": [
        "### Importing ```mxnet.gluon.nn```\n",
        "\n",
        "The module required to create a neural net is called ```mxnet.gluon.nn```. We can use the following import statement to import in the module."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cmpCHJup-_KT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from mxnet import nd\n",
        "from mxnet.gluon import nn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6y3FXnTHUmv_",
        "colab_type": "text"
      },
      "source": [
        "### Layers\n",
        "\n",
        "Neural networks (NNs) usually consist of many layers working together. Here we will examine how to create one layer first before creating a sequence of layers. \n",
        "\n",
        "A ```Dense``` layer is a common type of layer that you will encounter, especially when you are starting out with Machine Learning. It is also known as a Full-connected (FC) layer.\n",
        "\n",
        "The code block below shows by defining a dense layer with 2 output units."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n2q8HXTY_R9o",
        "colab_type": "code",
        "outputId": "a7f9e659-1cbf-47a8-b1d9-4f5a7e869cbe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "layer = nn.Dense(2)\n",
        "layer"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dense(None -> 2, linear)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bp8ViG0fVeMC",
        "colab_type": "text"
      },
      "source": [
        "Next, we will initialise its weight with the default initialisation method, which draws random values uniformly from $[-0.7, 0.7]$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZBQ86h4CVqv4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "layer.initialize()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9_Oo76xV8bz",
        "colab_type": "text"
      },
      "source": [
        "We will now create a random $3 \\times 4$ feature matrix, $X$, with random uniform values between $[-1,1]$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y79KNnGcVtiM",
        "colab_type": "code",
        "outputId": "4fa98f6c-3886-464d-b358-ad5a4cd4bf7b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "X = nd.random.uniform(-1, 1, (3, 4))\n",
        "display(X)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n",
              "[[ 0.09762704  0.18568921  0.43037868  0.6885315 ]\n",
              " [ 0.20552671  0.71589124  0.08976638  0.6945034 ]\n",
              " [-0.15269041  0.24712741  0.29178822 -0.23123658]]\n",
              "<NDArray 3x4 @cpu(0)>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XI8dkVpxWUEL",
        "colab_type": "text"
      },
      "source": [
        "Now, we will feed the feature matrix, $X$, into our layer to compute the output."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "enj9R2iZWOc-",
        "colab_type": "code",
        "outputId": "64cc6719-b3b2-4b29-8aad-4326c0ff7d9c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "layer(X)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\n",
              "[[-0.02524132 -0.00874885]\n",
              " [-0.06026538 -0.01308061]\n",
              " [ 0.02468396 -0.02181557]]\n",
              "<NDArray 3x2 @cpu(0)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SfZVc0wUWkb4",
        "colab_type": "text"
      },
      "source": [
        "We can see that output matrix is a $3 \\times 2$ matrix which was created from our $3 \\times 4$ input. Note that we did not specify the input size of ```layer``` before (although we can specify it with the argument ```in_units=4```), the system will automatically infer it during the first time we feed in data, create and initialise the weights.\n",
        "\n",
        "We can inspect the weights with the following line of code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nm5eN68bAlER",
        "colab_type": "code",
        "outputId": "f642f455-564c-4edb-fe12-3e31e6648c6b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "W = layer.weight.data()\n",
        "W"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\n",
              "[[-0.00873779 -0.02834515  0.05484822 -0.06206018]\n",
              " [ 0.06491279 -0.03182812 -0.01631819 -0.00312688]]\n",
              "<NDArray 2x4 @cpu(0)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBPJcrLpXTyJ",
        "colab_type": "text"
      },
      "source": [
        "Note that the shape of the weight matrix, $W$, is $(2,4)$. \n",
        "\n",
        "Recalling the rules of matrix-matrix product, the number of columns of the preceeding matrix must equal the number of rows of the succeeding matrix. However, we have a mismatch with our input of shape $(3, 4)$ and weight of shape $(2, 4)$. \n",
        "\n",
        "The resulting matrix is of shape $(3, 2)$. Intuitively, we can see that the equation is $X \\times W$ or more precisely $W^T$, where $W^T$ is the transpose matrix of $W$ with shape of $(4, 2)$.\n",
        "\n",
        "We can check this by manually using the ```nd.dot()``` function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z45XFEhNJZWI",
        "colab_type": "code",
        "outputId": "08627f23-abd6-4f86-9431-570babbeca8c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "print(layer(X))\n",
        "display(nd.dot(X,W.T))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[[-0.02524132 -0.00874885]\n",
            " [-0.06026538 -0.01308061]\n",
            " [ 0.02468396 -0.02181557]]\n",
            "<NDArray 3x2 @cpu(0)>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n",
              "[[-0.02524132 -0.00874885]\n",
              " [-0.06026538 -0.01308061]\n",
              " [ 0.02468396 -0.02181557]]\n",
              "<NDArray 3x2 @cpu(0)>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "teXME2tqD-74",
        "colab_type": "text"
      },
      "source": [
        "### Chaining layers into a neural network\n",
        "\n",
        "We can create a chain of layers by adding different layer types into ```nn.Sequential()```. You can think of ```nn.Sequential()``` as the entire model which contains all the layers needed to form our network and each layer is run sequentially, hence the name.\n",
        "\n",
        "The code below implements a famous network called [LeNet](http://yann.lecun.com/exdb/lenet/)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffUTaUblJxEs",
        "colab_type": "code",
        "outputId": "7cc18c69-9b86-45c5-a0b5-ac456cf3cb96",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "net = nn.Sequential()\n",
        "\n",
        "net.add(\n",
        "    nn.Conv2D(channels=6, kernel_size=5, activation=\"relu\", in_channels=3),\n",
        "    nn.MaxPool2D(pool_size=2, strides=2),\n",
        "    nn.Conv2D(channels=16, kernel_size=3, activation=\"relu\"),\n",
        "    nn.MaxPool2D(pool_size=2, strides=2),\n",
        "    nn.Dense(120, activation=\"relu\"),\n",
        "    nn.Dense(84, activation=\"relu\"),\n",
        "    nn.Dense(10)\n",
        ")\n",
        "\n",
        "net"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Conv2D(3 -> 6, kernel_size=(5, 5), stride=(1, 1), Activation(relu))\n",
              "  (1): MaxPool2D(size=(2, 2), stride=(2, 2), padding=(0, 0), ceil_mode=False, global_pool=False, pool_type=max, layout=NCHW)\n",
              "  (2): Conv2D(None -> 16, kernel_size=(3, 3), stride=(1, 1), Activation(relu))\n",
              "  (3): MaxPool2D(size=(2, 2), stride=(2, 2), padding=(0, 0), ceil_mode=False, global_pool=False, pool_type=max, layout=NCHW)\n",
              "  (4): Dense(None -> 120, Activation(relu))\n",
              "  (5): Dense(None -> 84, Activation(relu))\n",
              "  (6): Dense(None -> 10, linear)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e23TwrN3CRE_",
        "colab_type": "code",
        "outputId": "7697c45d-ea8f-48ec-bcab-c77d9e389401",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "net.initialize()\n",
        "# Input shape is (batch_size, color_channels, height, width\n",
        "X = nd.random.uniform(shape=(4, 3, 28, 28))\n",
        "Y = net(X)\n",
        "Y.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4, 10)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLlbRKjsOnzT",
        "colab_type": "text"
      },
      "source": [
        "We can index the network, ```net```, for a particular layer with the ```[]``` operators. Below we will examine the weight of the 1st layer and the bias of the 6th layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mc4y11ykGle4",
        "colab_type": "code",
        "outputId": "3ccfc693-a483-4f8f-f709-67f63f561e95",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(f\"First layer's weight: {net[0].weight.data().shape}\")\n",
        "print(f\"Last layer's bias: {net[5].bias.data().shape}\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First layer's weight: (6, 3, 5, 5)\n",
            "Last layer's bias: (84,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1vzCOFxcPczI",
        "colab_type": "text"
      },
      "source": [
        "Recall how we defined our first layer.\n",
        "\n",
        "```nn.Conv2D(channels=6, kernel_size=5, activation=\"relu\", in_channels=3),```\n",
        "\n",
        "The output tuple of ```(6, 3, 5, 5)``` contains four values which represents, respectively:\n",
        "\n",
        "1. the number of channels/filters/kernels\n",
        "2. the number of input channels to this layer\n",
        "3. the width of the kernel\n",
        "4. the height of the kernel\n",
        "\n",
        "Now recall how we defined our 6th layer.\n",
        "\n",
        "```nn.Dense(84, activation=\"relu\")```\n",
        "\n",
        "The output tuple of ```(84,)``` corresponds to the dimensionality of the output space. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "raDyGCc_2Ntn",
        "colab_type": "text"
      },
      "source": [
        "### Creating a neural network\n",
        "\n",
        "In this section, we will construct a neural network with a flexible forward function. The network will be a subclass of ```nn.Block```.\n",
        "\n",
        "We will implement two methods from ```nn.Block``` to define our neural network:\n",
        "1. ```__init__()``` method which creates the layers\n",
        "2. ```forward()``` method which defines the forward function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjjWRISAJk0-",
        "colab_type": "code",
        "outputId": "9ea9e076-de65-4d7b-aa0f-b8d97330b6fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "class MixMLP(nn.Block):\n",
        "    def __init__(self, **kwargs):\n",
        "        # Run `nn.Block`'s init method\n",
        "        super(MixMLP, self).__init__(**kwargs)\n",
        "        self.block = nn.Sequential()\n",
        "        self.block.add(nn.Dense(3, activation=\"relu\"),\n",
        "                     nn.Dense(4, activation=\"relu\"))\n",
        "        self.dense = nn.Dense(5)\n",
        "    def forward(self, x):\n",
        "        y = nd.relu(self.block(x))\n",
        "        print(y)\n",
        "        return self.dense(y)\n",
        "\n",
        "net = MixMLP()\n",
        "net"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MixMLP(\n",
              "  (block): Sequential(\n",
              "    (0): Dense(None -> 3, Activation(relu))\n",
              "    (1): Dense(None -> 4, Activation(relu))\n",
              "  )\n",
              "  (dense): Dense(None -> 5, linear)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fp2ZLAB19uad",
        "colab_type": "text"
      },
      "source": [
        "The above neural network consists of a sequential layer named ```block``` and a dense layer named ```dense```. \n",
        "\n",
        "The ```block``` layer has 2 dense layers which have ReLU as the activation function and are executed sequential as explained above. \n",
        "\n",
        "The ```dense``` layer has no activation function and is executed after **because we defined it as such in the forward function**. This means, if we did not call the ```dense``` layer in the forward function, no computation on the layer will happen and effectively using only the ```block``` layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lTL2a9oM_TrK",
        "colab_type": "text"
      },
      "source": [
        "We will now initialise the network, create a feature matrix, $X$, with random values between 0 and 1, and feed it into the network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2TXOwT7fPUIJ",
        "colab_type": "code",
        "outputId": "a6bc5e88-f6b6-4400-d0d4-98113b61fdd8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "net.initialize()\n",
        "X = nd.random.uniform(shape=(2,2))\n",
        "Y = net(X)\n",
        "Y"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[[0.00091355 0.         0.00028848 0.00129108]\n",
            " [0.0015378  0.         0.         0.00164204]]\n",
            "<NDArray 2x4 @cpu(0)>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\n",
              "[[-4.2858930e-05  1.1725605e-04  9.9832926e-07  6.2495194e-05\n",
              "   2.2238225e-06]\n",
              " [-8.8140361e-05  1.6045298e-04  2.3532573e-06  1.0022008e-04\n",
              "   1.5071701e-05]]\n",
              "<NDArray 2x5 @cpu(0)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7PXNvmxA_kH2",
        "colab_type": "text"
      },
      "source": [
        "### Manual computation\n",
        "\n",
        "To gain a deeper understanding of the network we just defined we will work backwards and manually compute, $Y$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJCB1J2M_34h",
        "colab_type": "text"
      },
      "source": [
        "As we need to know what the weights of each layer are, we will grab them from our network above and store them in three separate variables."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "THABbF9uUvOI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "W1 = net.block[0].weight.data()\n",
        "W2 = net.block[1].weight.data()\n",
        "W3 = net.dense.weight.data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLJFHFuZAZcp",
        "colab_type": "text"
      },
      "source": [
        "As explained previously explained, the output of each layer is the matrix-matrix (cross) product of the input, $X$, and the transpose matrix of the weights ($W_i.T$) generated at each layer.\n",
        "\n",
        "As such we will perform the steps as per the code below and yeild the same $Y$ as the network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GYhEYTe18A8c",
        "colab_type": "code",
        "outputId": "6d3c186f-95b0-46ec-fbe1-cb26a5beeb35",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "print(X)\n",
        "\n",
        "X = nd.relu(nd.dot(X, W1.T))\n",
        "X = nd.relu(nd.dot(X, W2.T))\n",
        "Y = nd.dot(X, W3.T)\n",
        "\n",
        "Y"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[[0.8268704  0.18115096]\n",
            " [0.5093421  0.7885455 ]]\n",
            "<NDArray 2x2 @cpu(0)>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\n",
              "[[-4.2858930e-05  1.1725605e-04  9.9832926e-07  6.2495194e-05\n",
              "   2.2238225e-06]\n",
              " [-8.8140361e-05  1.6045298e-04  2.3532573e-06  1.0022008e-04\n",
              "   1.5071701e-05]]\n",
              "<NDArray 2x5 @cpu(0)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5tPR7Y-a8Pls",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}