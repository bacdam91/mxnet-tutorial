{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Training the neural network",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bacdam91/mxnet-tutorial/blob/master/Training_the_neural_network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t9ACH7gVZTcu",
        "colab_type": "code",
        "outputId": "5ea40032-943f-40fc-ed77-471679e4088b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "!pip install mxnet"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: mxnet in /usr/local/lib/python3.6/dist-packages (1.5.1.post0)\n",
            "Requirement already satisfied: graphviz<0.9.0,>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from mxnet) (0.8.4)\n",
            "Requirement already satisfied: numpy<2.0.0,>1.16.0 in /usr/local/lib/python3.6/dist-packages (from mxnet) (1.17.4)\n",
            "Requirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.6/dist-packages (from mxnet) (2.21.0)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet) (2019.11.28)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet) (1.24.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z91uKlA14EZY",
        "colab_type": "text"
      },
      "source": [
        "# Training the neural network\n",
        "\n",
        "In this tutorial we will train the LeNet neural network with the FashionMNIST dataset.\n",
        "\n",
        "### Fashion MNIST dataset\n",
        "\n",
        "The Fashion MNIST dataset is a collection of $28 \\times 28$ pixels greyscale images of clothing items. Each item is associated with a scalar label, a number, to identify the particular clothing item. Between this and the handwritten digits MNIST dataset, they are considered the \"Hello World\" tutorials of the Machine Learning world.\n",
        "\n",
        "### Importing libraries\n",
        "\n",
        "We will be importing some libraries for this tutorial. Some of the new ones include:\n",
        "1. ```mxnet.init```: which will provide us with more weight initialisation methods\n",
        "2. ```mxnet.gluon.data.vision.datasets```: which will help us load the Fashion MNIST dataset\n",
        "3. ```mxnet.gluon.data.vision.transforms```: which will help us to transform the computer vision datasets\n",
        "4. ```time```: which will help us with benchmarking\n",
        "5. ```matplotlib.pyplot```: which will help us to output the dataset as drawings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9M4yhhzX3WK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from mxnet import nd, gluon, init, autograd\n",
        "from mxnet.gluon import nn\n",
        "from mxnet.gluon.data.vision import datasets, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1JVveew7xkj",
        "colab_type": "text"
      },
      "source": [
        "### Downloading the dataset\n",
        "\n",
        "Almost all Machine Learning libraries/frameworks provide a convenient method to download standard datasets, such the as Fashion MNIST. As such we will use MXNet's convenient method to download the dataset and inspect its first item.\n",
        "\n",
        "Notice that the parameter ```train``` is set to ```True```. We are saying we would like to load the training set only. If set to ```False``` we will load the testing set. We will do that later.\n",
        "\n",
        "__Note:__ As the Fashion MNIST is a standard dataset used for benchmarking, the dataset is of fixed sized and split for us. The reason for this is to prevent a more favourable randomisation and splitting. There are 60,000 examples in the training set and 10,000 examples in the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UulcEKdNZeob",
        "colab_type": "code",
        "outputId": "25749497-d061-4b62-b175-7dc9a51b680d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "mnist_train = datasets.FashionMNIST(train=True)\n",
        "X, y = mnist_train[0]\n",
        "(\"X shape: \", X.shape, \"X dtype: \", X.dtype, \"y: \", y)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('X shape: ', (28, 28, 1), 'X dtype: ', numpy.uint8, 'y: ', 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tCnc8wZ19B9r",
        "colab_type": "text"
      },
      "source": [
        "As we can see from the output, the ```X``` is a numpy array that is $28 \\times 28 \\times 1$. The dimensions stand for $height \\times width \\times channel$. The ```y``` is the label which tells us what the item is. In this case the first item is a pullover. All the items include:\n",
        "\n",
        "0. t-shirt\n",
        "1. trouser\n",
        "2. pullover\n",
        "3. dress\n",
        "4. coat\n",
        "5. sandal\n",
        "6. shirt\n",
        "7. sneaker\n",
        "8. bag\n",
        "9. ankle boot\n",
        "\n",
        "As the dataset stores these labels as integers, we will need an array of these text labels so that we can read the the network's prediction and easy evaluate the accuracy of the neural network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ml7T0gQq8_1a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text_labels = ['t-shirt', 'trouser', 'pullover', 'dress', 'coat',\n",
        "               'sandal', 'shirt', 'sneaker', 'bag', 'ankle boot']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npzgnnKt8cBf",
        "colab_type": "text"
      },
      "source": [
        "### Other datasets\n",
        "\n",
        "Beside the handwritten digits and the fashion MNIST datasets, there are a few other datasets that are considered as standard datasets for newcomers to learn and researchers to run their experiments, another popular dataset is the Zachary's Karate Club dataset which is used mainly in Graph Neural Networks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i862Wg5lm05P",
        "colab_type": "text"
      },
      "source": [
        "### Exploring the dataset with ```matplotlib```\n",
        "\n",
        "Let's explore the dataset some more and see its visual representations with ```matplotlib```.  We won't go into too much of the code of ```matplotlib``` in this tutorial. However, there are some videos on the basic usage of ```matplotlib``` on this channel."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S--2QXNTZ-_U",
        "colab_type": "code",
        "outputId": "2f980ad8-7a24-4cf9-c132-dfe84f01f083",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 135
        }
      },
      "source": [
        "X, y = mnist_train[0:10]\n",
        "# plot images\n",
        "fig, ax = plt.subplots(1, X.shape[0], figsize=(15, 15))\n",
        "for f, xi, yi in zip(ax, X, y):\n",
        "    # 3D->2D by removing the last channel dim\n",
        "    f.imshow(xi.reshape((28,28)).asnumpy())\n",
        "    f.set_title(text_labels[int(yi)])\n",
        "plt.show()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA2cAAAB2CAYAAABMKevGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 1080x1080 with 10 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhA0pC_2YIWm",
        "colab_type": "text"
      },
      "source": [
        "### Transforming the data\n",
        "\n",
        "In order to feed the data into a Gluon model, we need to transform the images into the (channel, height, width) format with floating point data type. We can do this with the ```transforms.ToTensor()``` method. In addition we will normalise all pixel values with ```transforms.Normalize()``` method. We can chain these two transforms together with ```transforms.Compose()``` which sequentially composes multiple transforms.\n",
        "\n",
        "If you would like to understand these transforms in more details, you can check my channel for more tutorials."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4VG0TygKaeSS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "transformer = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(0.13, 0.31)])\n",
        "mnist_train = mnist_train.transform_first(transformer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JYipDchpdzri",
        "colab_type": "text"
      },
      "source": [
        "### Creating the training data\n",
        "\n",
        "In order to use the data for training, we need to get a (randomised) batch of examples. We can achieve this with the ```gluon.data.Dataloader()``` method. We will define an arbitary batch size of 256 examples. To allow randomisation, we will set shuffle to ```True``` and number of workers to 4, which will allow us to process data in parallel. This is often necessary especially for complex data transforms. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NzLJMXqwal-6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 256\n",
        "train_data = gluon.data.DataLoader(\n",
        "    mnist_train, batch_size=batch_size, shuffle=True, num_workers=4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ki5JWKC__Ub",
        "colab_type": "text"
      },
      "source": [
        "The return object from ```gluon.data.DataLoader()``` is an iterable object but __does__ not support indexing so to examine a batch, we will use the ```for``` loop follows by a ```break``` statement immediately.\n",
        "\n",
        "We will also print out the number of batches generated with the method ```len()```."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wi5MJgMzaomq",
        "colab_type": "code",
        "outputId": "6b07e7f3-039f-45af-8eee-92f3611f615c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "for data, label in train_data:\n",
        "    print(data.shape, label.shape)\n",
        "    break\n",
        "\n",
        "print(len(train_data))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(256, 1, 28, 28) (256,)\n",
            "235\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3vwIllfAC_kE",
        "colab_type": "text"
      },
      "source": [
        "Notice that there are 235 batches and we defined 256 examples in each batch. If we multiple the 2 figures we get, $235 \\times 256 = 60,160$. As mentioned before, there are exactly 60,000 examples in the training set. If we remove the ```break``` statement and let the ```for``` loop print out the shape of all the mini-batches, we can see that the last batch has only 96 examples, which means $256 \\times 234 + 96 = 60,000$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3xRPWlFEnQc",
        "colab_type": "text"
      },
      "source": [
        "### Preparing the testing data\n",
        "\n",
        "Let's do the same method to get the test set ready for our neural network. \n",
        "\n",
        "Again, we can see that there are 40 batches: 256 examples in the first 39 batches and 16 examples in the last batch, $256 \\times 39 + 16 = 10,000$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f6a2AkJWapzK",
        "colab_type": "code",
        "outputId": "55e9dd07-b2c9-4493-ddf9-d99967212d26",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "mnist_valid = gluon.data.vision.FashionMNIST(train=False)\n",
        "valid_data = gluon.data.DataLoader(\n",
        "    mnist_valid.transform_first(transformer),\n",
        "    batch_size=batch_size, num_workers=4)\n",
        "\n",
        "for data, label in valid_data:\n",
        "    print(data.shape, label.shape)\n",
        "    break\n",
        "print(len(valid_data))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(256, 1, 28, 28) (256,)\n",
            "40\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jfsZQIhqFcOS",
        "colab_type": "text"
      },
      "source": [
        "# Define the model\n",
        "\n",
        "Let us reimplement the LeNet network introduced in the previous tutorial. The only difference here is that we will use the weight initialisation method ```Xavier``` instead, i.e., ```net.initialize(init=init.Xavier())```. This is a popular choice for deep convolutional neural networks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NSg7OIvearOr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "net = nn.Sequential()\n",
        "net.add(nn.Conv2D(channels=6, kernel_size=5, activation='relu'),\n",
        "        nn.MaxPool2D(pool_size=2, strides=2),\n",
        "        nn.Conv2D(channels=16, kernel_size=3, activation='relu'),\n",
        "        nn.MaxPool2D(pool_size=2, strides=2),\n",
        "        nn.Flatten(),\n",
        "        nn.Dense(120, activation=\"relu\"),\n",
        "        nn.Dense(84, activation=\"relu\"),\n",
        "        nn.Dense(10))\n",
        "net.initialize(init=init.Xavier())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RU8-o1FdGH83",
        "colab_type": "text"
      },
      "source": [
        "### Inspecting parameters of the neural network\n",
        "\n",
        "Once we have defined out neural network, we can have a look at the parameters, i.e., weights and biases, created with the method ```.collect_params()```."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3yXPDrtE4C3",
        "colab_type": "code",
        "outputId": "221dd17d-776e-4c4e-d7f9-ac4b0464a7af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "net.collect_params()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "sequential0_ (\n",
              "  Parameter conv0_weight (shape=(6, 0, 5, 5), dtype=<class 'numpy.float32'>)\n",
              "  Parameter conv0_bias (shape=(6,), dtype=<class 'numpy.float32'>)\n",
              "  Parameter conv1_weight (shape=(16, 0, 3, 3), dtype=<class 'numpy.float32'>)\n",
              "  Parameter conv1_bias (shape=(16,), dtype=<class 'numpy.float32'>)\n",
              "  Parameter dense0_weight (shape=(120, 0), dtype=float32)\n",
              "  Parameter dense0_bias (shape=(120,), dtype=float32)\n",
              "  Parameter dense1_weight (shape=(84, 0), dtype=float32)\n",
              "  Parameter dense1_bias (shape=(84,), dtype=float32)\n",
              "  Parameter dense2_weight (shape=(10, 0), dtype=float32)\n",
              "  Parameter dense2_bias (shape=(10,), dtype=float32)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4ZI0zZsGRVV",
        "colab_type": "text"
      },
      "source": [
        "### Accessing parameters of the neural network\n",
        "\n",
        "We can see that it outputs the layers defined in our neural network. These are indexed and each layer can be accessed like a dictionary by using the parameter's name as the key. However, initialisation does not actually happen until the first forward pass. As such if we try to access the ```shape``` properties of the layers not we will get a ```DeferredInitializationError``` thrown. We will have a look at those values a little later."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dNm00U6OG7U1",
        "colab_type": "text"
      },
      "source": [
        "### Creating a loss function\n",
        "\n",
        "Besides the neural network, we need to define the loss function and optimization method for training. We will use standard softmax cross entropy loss for classification problems. It first performs softmax on the output to obtain the predicted probability, and then compares the label with the cross entropy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mti6-KW6ath0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UIR2ECXEJ5ah",
        "colab_type": "text"
      },
      "source": [
        "### Creating a Trainer\n",
        "The optimization method we pick is the standard stochastic gradient descent (sgd) with constant learning rate of 0.1. We input these values into the ```gluon.Trainer()``` constructor, along with our list of parameters, i.e., ```net.collect_params()```. This will return a ```gluon.Trainer``` object which we will use for the training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mBzL4gBhavmS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': 0.1})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fosFDQyIJ86p",
        "colab_type": "text"
      },
      "source": [
        "The trainer is created with all parameters (both weights and gradients) in net. Later on, we only need to call the ```.step()``` method to update its weights."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FsrJTHslMOHB",
        "colab_type": "text"
      },
      "source": [
        "### Accuracy function\n",
        "\n",
        "We will create an auxillary function to calculate the model accuracy. This function compares the predicted proobability with the labels, calculates the mean and return the value as a scalar."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CY0H_tk7aw2S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def acc(output, label):\n",
        "    # output: (batch, num_output) float32 ndarray\n",
        "    # label: (batch, ) int32 ndarray\n",
        "    score = (output.argmax(axis=1) == label.astype('float32')).mean().asscalar()\n",
        "    return score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-AvOfDTVAxr",
        "colab_type": "text"
      },
      "source": [
        "### Traing the network\n",
        "\n",
        "Normally we would train the neural network for a few epochs and on the entire dataset. However, for demonstration purpose, we will train on 1 epoch and for 1 batch of 10 examples so that we can output the weights and gradients for explanation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJO5H1NkWwc8",
        "colab_type": "text"
      },
      "source": [
        "Let's reload the training and testing dataset with a batch size of 10. This is code is the same as what we have written previously."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l6IDhhLuVbwJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 10\n",
        "\n",
        "mnist_train = datasets.FashionMNIST(train=True)\n",
        "train_data = gluon.data.DataLoader(\n",
        "    mnist_train.transform_first(transformer), batch_size=batch_size, shuffle=True, num_workers=4)\n",
        "\n",
        "mnist_valid = gluon.data.vision.FashionMNIST(train=False)\n",
        "valid_data = gluon.data.DataLoader(\n",
        "    mnist_valid.transform_first(transformer), batch_size=batch_size, num_workers=4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YAPdkkw2W-Yc",
        "colab_type": "text"
      },
      "source": [
        "Let's run our neural network and output the results. We will go through and explain what is happening after."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VpNVNYsmayDy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "006ab3a3-a264-4040-ccea-131afc3ce851"
      },
      "source": [
        "for epoch in range(1):\n",
        "    train_loss, train_acc, valid_acc = 0., 0., 0.\n",
        "    tic = time.time()\n",
        "    for data, label in train_data:\n",
        "        # forward + backward\n",
        "        with autograd.record():\n",
        "            output = net(data)\n",
        "            loss = softmax_cross_entropy(output, label)\n",
        "        print(\"Shape of first convolutional layer:\", net.collect_params()[\"conv0_weight\"].data().shape)\n",
        "        print(\"Weight values of first convolutional layer:\", net.collect_params()[\"conv0_weight\"].data())\n",
        "        print(\"Grad before backward()\", net.collect_params()[\"conv0_weight\"].grad())\n",
        "        loss.backward()\n",
        "        print(\"Grad after backward(): \", net.collect_params()[\"conv0_weight\"].grad())\n",
        "        # update parameters\n",
        "        trainer.step(batch_size)\n",
        "        print(\"Weight values of first convolutional layer:\", net.collect_params()[\"conv0_weight\"].data())\n",
        "        # calculate training metrics\n",
        "        train_loss += loss.mean().asscalar()\n",
        "        train_acc += acc(output, label)\n",
        "        break\n",
        "    # calculate validation accuracy\n",
        "    for data, label in valid_data:\n",
        "        valid_acc += acc(net(data), label)\n",
        "        break\n",
        "    # print(\"Epoch %d: loss %.3f, train acc %.3f, test acc %.3f, in %.1f sec\" % (\n",
        "    #         epoch, train_loss/len(train_data), train_acc/len(train_data),\n",
        "    #         valid_acc/len(valid_data), time.time()-tic))\n",
        "\n",
        "    print(\"Epoch %d: loss %.3f, train acc %.3f, test acc %.3f, in %.1f sec\" % (\n",
        "            epoch, train_loss/batch_size, train_acc/batch_size,\n",
        "            valid_acc/batch_size, time.time()-tic))\n",
        "    "
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of first convolutional layer: (6, 1, 5, 5)\n",
            "Weight values of first convolutional layer: \n",
            "[[[[ 0.01807702  0.03438295  0.07969065  0.12749125  0.03805615]\n",
            "   [ 0.13255729  0.0166215   0.12859704 -0.02827276  0.04575911]\n",
            "   [ 0.05402867 -0.0428167  -0.02311321 -0.07497861  0.14508452]\n",
            "   [-0.16416161  0.17170732 -0.08419175 -0.04316488 -0.00827123]\n",
            "   [ 0.10803397  0.11560483  0.0107006  -0.00741501  0.0251988 ]]]\n",
            "\n",
            "\n",
            " [[[-0.0397048   0.15761037  0.1244594  -0.15885738 -0.06021675]\n",
            "   [-0.1528976   0.05487221 -0.17767657 -0.04879385  0.12317847]\n",
            "   [ 0.16929738  0.10300924 -0.13318819  0.13702588  0.13705368]\n",
            "   [ 0.17724578 -0.00977369  0.11078681  0.1114357  -0.01426527]\n",
            "   [ 0.00758338  0.10388784  0.06624411 -0.14136368  0.08170648]]]\n",
            "\n",
            "\n",
            " [[[-0.03072909  0.18412147  0.08159234  0.16018812 -0.18512166]\n",
            "   [-0.13771594 -0.07320179  0.18480872 -0.13081619 -0.09773365]\n",
            "   [-0.15096845 -0.03829905 -0.11618664 -0.04150979 -0.05719319]\n",
            "   [ 0.06286173 -0.0382299   0.16129233  0.01437493  0.12824865]\n",
            "   [-0.02992454 -0.06915005  0.068592    0.00909087 -0.10944962]]]\n",
            "\n",
            "\n",
            " [[[-0.02094097  0.14002748 -0.10014514 -0.17502162  0.01274444]\n",
            "   [ 0.06312889  0.15330173 -0.03062434 -0.01584825  0.02173449]\n",
            "   [-0.02566426 -0.1331748   0.16262133 -0.11180148  0.10309534]\n",
            "   [ 0.11137415  0.07997994  0.1734104   0.1121196  -0.06909426]\n",
            "   [-0.15079728  0.07122247  0.00672239  0.13938744  0.13517724]]]\n",
            "\n",
            "\n",
            " [[[-0.02370289 -0.11662294 -0.17556281  0.15981166  0.0183914 ]\n",
            "   [ 0.1658072  -0.02395193 -0.00564784 -0.02949004 -0.06646039]\n",
            "   [-0.06283177 -0.1279755  -0.10937689  0.07364441  0.04416938]\n",
            "   [-0.14074297 -0.07419349 -0.00548978 -0.0863504   0.04915652]\n",
            "   [ 0.04485925  0.11784826  0.01079214  0.06777968 -0.1353253 ]]]\n",
            "\n",
            "\n",
            " [[[-0.00053284  0.00502835  0.03214337 -0.11686076  0.08138116]\n",
            "   [ 0.10566761 -0.08943494  0.13108699  0.01711187 -0.00213426]\n",
            "   [-0.03432651  0.12834145 -0.11962165 -0.15566906  0.17391805]\n",
            "   [ 0.00194278 -0.07516979 -0.1609866  -0.07855812 -0.02661832]\n",
            "   [-0.14213437 -0.14941591 -0.11786541 -0.13807312 -0.00211467]]]]\n",
            "<NDArray 6x1x5x5 @cpu(0)>\n",
            "Grad before backward() \n",
            "[[[[0. 0. 0. 0. 0.]\n",
            "   [0. 0. 0. 0. 0.]\n",
            "   [0. 0. 0. 0. 0.]\n",
            "   [0. 0. 0. 0. 0.]\n",
            "   [0. 0. 0. 0. 0.]]]\n",
            "\n",
            "\n",
            " [[[0. 0. 0. 0. 0.]\n",
            "   [0. 0. 0. 0. 0.]\n",
            "   [0. 0. 0. 0. 0.]\n",
            "   [0. 0. 0. 0. 0.]\n",
            "   [0. 0. 0. 0. 0.]]]\n",
            "\n",
            "\n",
            " [[[0. 0. 0. 0. 0.]\n",
            "   [0. 0. 0. 0. 0.]\n",
            "   [0. 0. 0. 0. 0.]\n",
            "   [0. 0. 0. 0. 0.]\n",
            "   [0. 0. 0. 0. 0.]]]\n",
            "\n",
            "\n",
            " [[[0. 0. 0. 0. 0.]\n",
            "   [0. 0. 0. 0. 0.]\n",
            "   [0. 0. 0. 0. 0.]\n",
            "   [0. 0. 0. 0. 0.]\n",
            "   [0. 0. 0. 0. 0.]]]\n",
            "\n",
            "\n",
            " [[[0. 0. 0. 0. 0.]\n",
            "   [0. 0. 0. 0. 0.]\n",
            "   [0. 0. 0. 0. 0.]\n",
            "   [0. 0. 0. 0. 0.]\n",
            "   [0. 0. 0. 0. 0.]]]\n",
            "\n",
            "\n",
            " [[[0. 0. 0. 0. 0.]\n",
            "   [0. 0. 0. 0. 0.]\n",
            "   [0. 0. 0. 0. 0.]\n",
            "   [0. 0. 0. 0. 0.]\n",
            "   [0. 0. 0. 0. 0.]]]]\n",
            "<NDArray 6x1x5x5 @cpu(0)>\n",
            "Grad after backward():  \n",
            "[[[[ 2.8582318   2.9455712   3.2443004   2.9257858   2.502559  ]\n",
            "   [ 2.8188472   3.0294979   3.1426039   2.5850542   1.9819096 ]\n",
            "   [ 2.795858    3.1508877   2.8626304   2.715171    2.271888  ]\n",
            "   [ 3.025729    3.8767042   3.005041    2.8265266   1.8410603 ]\n",
            "   [ 2.8474188   3.188648    2.5114717   2.5859878   2.0486932 ]]]\n",
            "\n",
            "\n",
            " [[[ 0.39616996  1.2164233   1.2083578   0.72633195  1.1329367 ]\n",
            "   [ 0.6942717   1.7116095   1.8026013   1.3892071   1.5204217 ]\n",
            "   [ 1.5206068   1.724256    1.7151957   1.7040871   2.1207197 ]\n",
            "   [ 1.5203775   1.6050009   1.5590228   1.960377    2.166049  ]\n",
            "   [ 1.5090381   1.3017399   1.4574761   1.4751321   1.9844685 ]]]\n",
            "\n",
            "\n",
            " [[[-0.05631013  0.02364163 -0.0292227  -0.1360874  -0.21776158]\n",
            "   [ 0.08055468  0.20990863  0.09710796  0.26986122 -0.38234115]\n",
            "   [ 0.02685654  0.30463433  0.31093496  0.04915868 -0.32177615]\n",
            "   [ 0.42339703  0.49150503  0.38278198  0.50506794  0.17192873]\n",
            "   [ 0.3911172   0.74370366  0.31287557  0.46490258 -0.05672058]]]\n",
            "\n",
            "\n",
            " [[[-1.6231247  -1.497134   -2.019667   -0.47278252 -0.10517281]\n",
            "   [-1.5762141  -1.3680835  -1.7278585  -0.8850109  -0.59118176]\n",
            "   [-1.9045982  -1.6603894  -1.9406799  -1.005816   -0.54824436]\n",
            "   [-1.3887854  -1.5721629  -1.6155269  -1.2206745  -0.8737784 ]\n",
            "   [-1.557508   -1.6798573  -1.9841874  -1.3934352  -1.2631669 ]]]\n",
            "\n",
            "\n",
            " [[[-0.51823807 -0.3868876  -0.32568085  0.0765726   0.2523039 ]\n",
            "   [-0.39734071 -0.29697135 -0.12094859  0.2712255   0.44870368]\n",
            "   [-0.1523684  -0.21477526  0.01067488  0.40083024  0.61394376]\n",
            "   [-0.08739444 -0.05890143  0.03486757  0.48836136  0.6245724 ]\n",
            "   [ 0.07724902  0.28779495  0.2827465   0.49880663  0.7504916 ]]]\n",
            "\n",
            "\n",
            " [[[ 0.35250774 -0.04720727  0.03378685 -0.00712709 -0.1669667 ]\n",
            "   [ 0.1844033  -0.12734301  0.07380292 -0.1962212  -0.47019482]\n",
            "   [ 0.0555397  -0.3080694  -0.05540671 -0.411163   -0.42205998]\n",
            "   [ 0.67640805  0.21008039  0.12236316 -0.10265785 -0.12450004]\n",
            "   [ 0.54466605  0.27910873  0.1563469   0.08145761  0.18637112]]]]\n",
            "<NDArray 6x1x5x5 @cpu(0)>\n",
            "Weight values of first convolutional layer: \n",
            "[[[[-0.0105053   0.00492724  0.04724764  0.09823339  0.01303056]\n",
            "   [ 0.10436881 -0.01367348  0.09717099 -0.0541233   0.02594001]\n",
            "   [ 0.02607009 -0.07432558 -0.05173951 -0.10213032  0.12236563]\n",
            "   [-0.1944189   0.13294028 -0.11424216 -0.07143015 -0.02668184]\n",
            "   [ 0.07955978  0.08371835 -0.01441412 -0.03327489  0.00471187]]]\n",
            "\n",
            "\n",
            " [[[-0.0436665   0.14544614  0.11237583 -0.1661207  -0.07154612]\n",
            "   [-0.15984032  0.03775612 -0.19570258 -0.06268592  0.10797425]\n",
            "   [ 0.15409131  0.08576667 -0.15034014  0.11998501  0.11584648]\n",
            "   [ 0.162042   -0.0258237   0.09519658  0.09183192 -0.03592576]\n",
            "   [-0.007507    0.09087044  0.05166935 -0.156115    0.06186179]]]\n",
            "\n",
            "\n",
            " [[[-0.03016598  0.18388505  0.08188456  0.161549   -0.18294404]\n",
            "   [-0.13852148 -0.07530088  0.18383764 -0.1335148  -0.09391024]\n",
            "   [-0.15123701 -0.0413454  -0.11929599 -0.04200138 -0.05397543]\n",
            "   [ 0.05862775 -0.04314495  0.1574645   0.00932425  0.12652937]\n",
            "   [-0.03383571 -0.07658709  0.06546324  0.00444184 -0.10888241]]]\n",
            "\n",
            "\n",
            " [[[-0.00470973  0.15499882 -0.07994847 -0.1702938   0.01379617]\n",
            "   [ 0.07889103  0.16698256 -0.01334576 -0.00699814  0.02764631]\n",
            "   [-0.00661827 -0.11657091  0.18202813 -0.10174333  0.10857778]\n",
            "   [ 0.125262    0.09570158  0.18956567  0.12432635 -0.06035648]\n",
            "   [-0.1352222   0.08802104  0.02656427  0.1533218   0.14780891]]]\n",
            "\n",
            "\n",
            " [[[-0.01852051 -0.11275406 -0.172306    0.15904593  0.01586836]\n",
            "   [ 0.16978061 -0.02098222 -0.00443835 -0.0322023  -0.07094743]\n",
            "   [-0.06130809 -0.12582774 -0.10948364  0.06963611  0.03802994]\n",
            "   [-0.13986903 -0.07360448 -0.00583846 -0.09123401  0.04291079]\n",
            "   [ 0.04408675  0.11497031  0.00796467  0.06279161 -0.14283021]]]\n",
            "\n",
            "\n",
            " [[[-0.00405791  0.00550043  0.0318055  -0.11678949  0.08305082]\n",
            "   [ 0.10382357 -0.08816151  0.13034897  0.01907408  0.00256769]\n",
            "   [-0.0348819   0.13142215 -0.11906758 -0.15155743  0.17813866]\n",
            "   [-0.0048213  -0.0772706  -0.16221024 -0.07753155 -0.02537332]\n",
            "   [-0.14758103 -0.152207   -0.11942887 -0.13888769 -0.00397838]]]]\n",
            "<NDArray 6x1x5x5 @cpu(0)>\n",
            "Epoch 0: loss 0.246, train acc 0.010, test acc 0.010, in 0.1 sec\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JjkfFm75JsCb",
        "colab_type": "text"
      },
      "source": [
        "### Running the network\n",
        "\n",
        "From the code we can see that we are running the neural network for 1 epoch. However, if you look inside the two ```for``` loops, the first ```for``` loop is for training and second ```for``` loop is for testing, on line 20 and 24, there are two ```break``` statements. These two ```break``` statements allow us to escape the loop and run only 1 batch of 10 examples. Needless to say our accuracy scores (0.01 for both training and test accuracy) won't be very good but that is not the purpose here.\n",
        "\n",
        "On line 2 we initialise the training loss, train accuracy and test accuracy to 0 and we start the timer for each epoch on line 3.\n",
        "\n",
        "#### Forward pass\n",
        "\n",
        "From line 4 we start the training. The ```for``` loop loops through each batch of 10 examples. Within the ```autograd.record()``` scope we feed in the data, store the output in a variable, and calculate the loss with our loss function.\n",
        "\n",
        "#### Layers initialization\n",
        "The first time this runs, it initialises the weights and biases. Recalling what we talked about earlier regarding the ```.collect_params()``` method, we need to feed in a dataset before the network actually initialise we can access the weights and biases.\n",
        "\n",
        "Let's have a look at the shape, the weight values and gradients of the first convolution layer. As we have not run the back propagation, we expect the gradient to 0s.\n",
        "\n",
        "#### Back propagation\n",
        "After running back propagation, i.e., ```loss.backward()```, we can see that the gradients have been calculated. \n",
        "\n",
        "#### Updating parameters\n",
        "To update the parameters in each layers, we need to call the ```trainer.step(batch_size)```. We will output this and see that the weights have been updated and are not the same as previously.\n",
        "\n",
        "#### Storing loss and accuracy\n",
        "Line 18 and 19 are where we store the loss and accuracy score.\n",
        "\n",
        "#### Testing the network\n",
        "Line 22 is where we test the network. It is very similar to how we train the network.\n",
        "\n",
        "#### Output the result\n",
        "From line 25, the code is there for outputing the results. The commented out ```print()``` statement is what we would actually run when we train over the entire dataset and test over the entire test data.\n",
        "\n",
        "However, since we only trained and tested over a batch, we will use the second ```print()``` statement."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "14w35dyyJsfO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}