{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Automatic differentiation with autograd",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bacdam91/mxnet-tutorial/blob/master/Automatic_differentiation_with_autograd.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QACxfDpNDwAa",
        "colab_type": "code",
        "outputId": "184dc6d4-4ae7-469b-de09-e63becd87378",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "!pip install mxnet"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: mxnet in /usr/local/lib/python3.6/dist-packages (1.5.1.post0)\n",
            "Requirement already satisfied: numpy<2.0.0,>1.16.0 in /usr/local/lib/python3.6/dist-packages (from mxnet) (1.17.4)\n",
            "Requirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.6/dist-packages (from mxnet) (2.21.0)\n",
            "Requirement already satisfied: graphviz<0.9.0,>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from mxnet) (0.8.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet) (2019.11.28)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet) (3.0.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DeTt2zY47lS8",
        "colab_type": "text"
      },
      "source": [
        "# Automatic differentiation with ```autograd```\n",
        "\n",
        "### Minimising loss\n",
        "\n",
        "The purpose of training a neural network is so that it can get better at predicting an outcome, $Y$, from the features, $X$, we present to it. One of the ways to measure if the network is getting better is through minimising the loss or error.\n",
        "\n",
        "To achieve this goal, we can compute the gradient of the loss function with respect to weights and then update the weights accordingly. We can do this by hand or we can use the ```autograd``` package to expedite this process by automatically calculating the derivatives."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "snkbSSjz9KM7",
        "colab_type": "text"
      },
      "source": [
        "### Basic usage\n",
        "\n",
        "Let's first import the necessary packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MaWMoIdiD3mw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from mxnet import nd\n",
        "from mxnet import autograd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7C6XVUr49Vrc",
        "colab_type": "text"
      },
      "source": [
        "As an example, we would like to find the derivative of the function $f(x) = 2x^2$. Recalling our high school mathematics, we know that the derivative is $f'(x) = 4x$. Using this simple function as an example will help us evaluate and understand the ```autograd``` package in depth.\n",
        "\n",
        "Firstly let us assign an initial value of ```X```.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ADMGsPQCEBrv",
        "colab_type": "code",
        "outputId": "89c9393b-dbdc-4f05-85a1-02d151b491bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "X = nd.array([[1, 2], [3, 4]])\n",
        "X"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\n",
              "[[1. 2.]\n",
              " [3. 4.]]\n",
              "<NDArray 2x2 @cpu(0)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9s8oW0OZN2Lh",
        "colab_type": "text"
      },
      "source": [
        "Before we calculate the gradient of $f(x)$, we need to specify the storage by invoking the ```.attach_grad()``` method on the NDArray ```X```."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWlpgZLMEGPb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X.attach_grad()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHby6AFDOVfN",
        "colab_type": "text"
      },
      "source": [
        "We can check the gradient of each value in ```X``` by accessing the ```.grad``` attribute. Since we have not actually calculated anything, we expect to see zeros."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j9MPKtOtEIFP",
        "colab_type": "code",
        "outputId": "d7ca5e55-6adc-4c81-854b-89a94f9c237f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "X.grad"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\n",
              "[[0. 0.]\n",
              " [0. 0.]]\n",
              "<NDArray 2x2 @cpu(0)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-39b3ZoEOmrY",
        "colab_type": "text"
      },
      "source": [
        "Now we are going to define the function $y = f(x)$. To let MXNet store $Y$, so that we can compute gradients later, we need to put the definition inside an ```autograd.record()``` scope."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tEFsPXo1EI5V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "f4763338-9f47-4084-f8e5-6a39c4cd0efb"
      },
      "source": [
        "with autograd.record():\n",
        "    Y = 2 * X * X\n",
        "Y"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\n",
              "[[ 2.  8.]\n",
              " [18. 32.]]\n",
              "<NDArray 2x2 @cpu(0)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQ1LwEhTP4gM",
        "colab_type": "text"
      },
      "source": [
        "Outputing ```Y``` we can see that the values in ```X``` are plugged into $f(x)$. We can manually evaluate this to understand what has happened.\n",
        "\n",
        "1. $f(1) = 2 \\times 1^{2} = 2$\n",
        "2. $f(2) = 2 \\times 2^{2} = 2 \\times 4 = 8$\n",
        "3. $f(3) = 2 \\times 3^{2} = 2 \\times 9 = 18$\n",
        "4. $f(4) = 2 \\times 4^{2} = 2 \\times 16 = 32$\n",
        "\n",
        "We can see that ```Y``` now stores the value results of $f(x)$ over the values in ```X```.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WveSrHdQQmD3",
        "colab_type": "text"
      },
      "source": [
        "To calculate the gradient of $f(x)$, in other words, calculating $f'(x)$, we need to call the ```.backward()``` method on ```Y```, which invoke back propagation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FXGORqhWEYpp",
        "colab_type": "code",
        "outputId": "1111fec4-7320-461a-f395-0f386ba535ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        }
      },
      "source": [
        "print(\"Before back-prop\")\n",
        "print(X)\n",
        "print(X.grad)\n",
        "print(Y)\n",
        "Y.backward()\n",
        "print(\"After back-prop\")\n",
        "print(X)\n",
        "print(X.grad)\n",
        "print(Y)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Before back-prop\n",
            "\n",
            "[[1. 2.]\n",
            " [3. 4.]]\n",
            "<NDArray 2x2 @cpu(0)>\n",
            "\n",
            "[[0. 0.]\n",
            " [0. 0.]]\n",
            "<NDArray 2x2 @cpu(0)>\n",
            "\n",
            "[[ 2.  8.]\n",
            " [18. 32.]]\n",
            "<NDArray 2x2 @cpu(0)>\n",
            "After back-prop\n",
            "\n",
            "[[1. 2.]\n",
            " [3. 4.]]\n",
            "<NDArray 2x2 @cpu(0)>\n",
            "\n",
            "[[ 4.  8.]\n",
            " [12. 16.]]\n",
            "<NDArray 2x2 @cpu(0)>\n",
            "\n",
            "[[ 2.  8.]\n",
            " [18. 32.]]\n",
            "<NDArray 2x2 @cpu(0)>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8sQSu7BsRiCn",
        "colab_type": "text"
      },
      "source": [
        "We can see that before calling the ```.backward()``` method on ```Y```, the values of ```X.grad``` are zeros. After invoking back propagation, ```X.grad``` gives us the gradient at each $x$ value. \n",
        "\n",
        "We can manually check this to confirm the values are as we expected.\n",
        "\n",
        "$f(x) = 2x^{2}$\n",
        "\n",
        "$f'(x) = 4x$\n",
        "\n",
        "1. $f'(1) = 4 \\times 1 = 4$\n",
        "2. $f'(2) = 4 \\times 2 = 8$\n",
        "3. $f'(3) = 4 \\times 3 = 12$\n",
        "4. $f'(4) = 4 \\times 4 = 16$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Prt2WUPvceEb",
        "colab_type": "text"
      },
      "source": [
        "### Using Python control flows\n",
        "\n",
        "Sometimes we want to write dynamic programs where the execution depends on some real-time values. MXNet will record the execution trace and compute the gradient as well.\n",
        "\n",
        "Consider the following function f(a): it doubles the inputs until its norm reaches 1000. Then it selects one element depending on the sum of its elements.\n",
        "\n",
        "We will ask it to output ```b``` during the calculation and tell us when its norm is greater than 1000."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pvgV19nXS0fG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def f(a):\n",
        "    b = a * 2\n",
        "    while b.norm().asscalar() < 1000:\n",
        "        b = b * 2\n",
        "        print(b)\n",
        "        print(b.sum().asscalar())\n",
        "        print(b.norm().asscalar())\n",
        "    \n",
        "    print(\"Done\")\n",
        "    if b.sum().asscalar() >= 0:\n",
        "        c = b[0]\n",
        "    else:\n",
        "        c = b[1]\n",
        "    return c"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTuG_LICia6R",
        "colab_type": "text"
      },
      "source": [
        "We record the trace and feed in a random value. We will let the range be between -1 and 1 so the simulation can produce a sum of ```b``` of less than 0 while the norm (```b.norm()``` is greater than 1000."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wV5-w1XsS9Cr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 901
        },
        "outputId": "27a5c9cc-6bb9-4ddf-b954-a646890ab2d4"
      },
      "source": [
        "a = nd.random.uniform(-1, 1, shape=2)\n",
        "print(a)\n",
        "a.attach_grad()\n",
        "with autograd.record():\n",
        "    c = f(a)\n",
        "c.backward()\n",
        "c"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[ 0.9273255  -0.45468742]\n",
            "<NDArray 2 @cpu(0)>\n",
            "\n",
            "[ 3.709302  -1.8187497]\n",
            "<NDArray 2 @cpu(0)>\n",
            "1.8905523\n",
            "4.1311946\n",
            "\n",
            "[ 7.418604  -3.6374993]\n",
            "<NDArray 2 @cpu(0)>\n",
            "3.7811046\n",
            "8.262389\n",
            "\n",
            "[14.837208  -7.2749987]\n",
            "<NDArray 2 @cpu(0)>\n",
            "7.562209\n",
            "16.524778\n",
            "\n",
            "[ 29.674416 -14.549997]\n",
            "<NDArray 2 @cpu(0)>\n",
            "15.124418\n",
            "33.049557\n",
            "\n",
            "[ 59.34883  -29.099995]\n",
            "<NDArray 2 @cpu(0)>\n",
            "30.248837\n",
            "66.09911\n",
            "\n",
            "[118.69766 -58.19999]\n",
            "<NDArray 2 @cpu(0)>\n",
            "60.497673\n",
            "132.19823\n",
            "\n",
            "[ 237.39532 -116.39998]\n",
            "<NDArray 2 @cpu(0)>\n",
            "120.995346\n",
            "264.39645\n",
            "\n",
            "[ 474.79065 -232.79996]\n",
            "<NDArray 2 @cpu(0)>\n",
            "241.99069\n",
            "528.7929\n",
            "\n",
            "[ 949.5813 -465.5999]\n",
            "<NDArray 2 @cpu(0)>\n",
            "483.98138\n",
            "1057.5858\n",
            "Done\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\n",
              "[949.5813]\n",
              "<NDArray 1 @cpu(0)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14TCaUs0i_Kg",
        "colab_type": "text"
      },
      "source": [
        "We know that ```b``` is a linear function of ```a```, and ```c``` is chosen from ```b```. Then the gradient with respect to a be will be either ```[c/a[0], 0]``` or ```[0, c/a[1]]```, depending on which element from b we picked. Letâ€™s find the results:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0vRMlhtCS-sE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "dc887596-05ac-4064-e565-1b6984e3305f"
      },
      "source": [
        "[a.grad, c/a]"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\n",
              " [1024.    0.]\n",
              " <NDArray 2 @cpu(0)>, \n",
              " [ 1024.     -2088.4265]\n",
              " <NDArray 2 @cpu(0)>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LaKi6USOX_oK",
        "colab_type": "text"
      },
      "source": [
        "### A quick side note\n",
        "\n",
        "The ```.norm()``` method calculates, by default, the $l_2$ norm of a matrix. The resulting value is an array of a single value, i.e., a scalar, but it requires explicit conversion to a scalar with the method ```.asscalar()```.\n",
        "\n",
        "The section below is a manual calculation of the $l_2$ norm and is optional but will help us to gain a better understanding of the $l_2$ norm calculation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oiU_2MbmYs2A",
        "colab_type": "text"
      },
      "source": [
        "### The $l_2$ norm\n",
        "\n",
        "The $l_2$ norm is defined according to the following equation:\n",
        "\n",
        "$||X||_{2} = (\\sum^{n}_{i = 1} x_i^2)^{\\frac{1}{2}}$\n",
        "\n",
        "The equation may look complicate it but it is quite simple. Basically it is asking us to find the square root of the sum of all values in $X$ squared. This will make more sense when we go to calculate it by manually.\n",
        "\n",
        "Let's define a matrix, $X$, and calculate its norm.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F3ueskGxYq8o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = nd.array([[2, 4], [7, 5]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0nK5LW-ZpBH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "04504a0a-e5a1-41ff-a5aa-600bb7ccc605"
      },
      "source": [
        "X.norm()"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\n",
              "[9.695359]\n",
              "<NDArray 1 @cpu(0)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OoHLNlXYZ1W2",
        "colab_type": "text"
      },
      "source": [
        "Now let us manually work out the norm according the equation for $l_2$ norm above.\n",
        "\n",
        "1. when $i = 1$, $x = 2$, i.e., ```X[0,0]```, hence the $x^2$ equals $4$.\n",
        "2. when $i = 2$, $x = 3$, i.e., ```X[0,1]```, hence the $x^2$ equals $16$.\n",
        "3. when $i = 3$, $x = 7$, i.e., ```X[1,0]```, hence the $x^2$ equals $49$.\n",
        "4. when $i = 4$, $x = 5$, i.e., ```X[1,1]```, hence the $x^2$ equals $25$.\n",
        "5. the sum of those values is $4 + 16 + 49 + 25 = 94$\n",
        "6. the square root of the sum is $\\sqrt{94} \\approx 9.695359$, which is the same as the value calculated by the ```.norm()``` method.\n",
        "\n",
        "Putting it all together:\n",
        "\n",
        "$||X||_2 = \\sqrt{2^2 + 4^2 + 7^2 + 5^2} = \\sqrt{94} \\approx 9.695359$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTnKVrRujOvv",
        "colab_type": "text"
      },
      "source": [
        "### The sum\n",
        "\n",
        "The method ```.sum()``` simplies add all the values together. Consider the same matrix $X$, the sum of $X$, e.g., ```X.sum()``` equals 18.\n",
        "\n",
        "$sum(X) = 2 + 4 + 7 + 5 = 18$\n",
        "\n",
        "We can check by calling the ```.sum()``` method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_o-rCsRfcrUF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "9b1d5e6d-baab-486e-d48d-87f6d5fe10fd"
      },
      "source": [
        "X.sum()"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\n",
              "[18.]\n",
              "<NDArray 1 @cpu(0)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "deFptHKcjoHv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}