{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Automatic differentiation with autograd",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bacdam91/mxnet-tutorial/blob/master/Automatic_differentiation_with_autograd.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QACxfDpNDwAa",
        "colab_type": "code",
        "outputId": "3706fcbd-9724-435a-fc16-c747117656d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "\n",
        "!pip install mxnet"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting mxnet\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/92/6c/c6e5562f8face683cec73f5d4d74a58f8572c0595d54f1fed9d923020bbd/mxnet-1.5.1.post0-py2.py3-none-manylinux1_x86_64.whl (25.4MB)\n",
            "\u001b[K     |████████████████████████████████| 25.4MB 113kB/s \n",
            "\u001b[?25hCollecting graphviz<0.9.0,>=0.8.1\n",
            "  Downloading https://files.pythonhosted.org/packages/53/39/4ab213673844e0c004bed8a0781a0721a3f6bb23eb8854ee75c236428892/graphviz-0.8.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: numpy<2.0.0,>1.16.0 in /usr/local/lib/python3.6/dist-packages (from mxnet) (1.17.4)\n",
            "Requirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.6/dist-packages (from mxnet) (2.21.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet) (2019.11.28)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet) (2.8)\n",
            "Installing collected packages: graphviz, mxnet\n",
            "  Found existing installation: graphviz 0.10.1\n",
            "    Uninstalling graphviz-0.10.1:\n",
            "      Successfully uninstalled graphviz-0.10.1\n",
            "Successfully installed graphviz-0.8.4 mxnet-1.5.1.post0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DeTt2zY47lS8",
        "colab_type": "text"
      },
      "source": [
        "# Automatic differentiation with ```autograd```\n",
        "\n",
        "### Minimising loss\n",
        "\n",
        "The purpose of training a neural network is so that it can get better at predicting an outcome, $Y$, from the features, $X$, we present to it. One of the ways to measure if the network is getting better is through minimising the loss or error.\n",
        "\n",
        "To achieve this goal, we can compute the gradient of the loss function with respect to weights and then update the weights accordingly. We can do this by hand or we can use the ```autograd``` package to expedite this process by automatically calculating the derivatives."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "snkbSSjz9KM7",
        "colab_type": "text"
      },
      "source": [
        "### Basic usage\n",
        "\n",
        "Let's first import the necessary packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MaWMoIdiD3mw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from mxnet import nd\n",
        "from mxnet import autograd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7C6XVUr49Vrc",
        "colab_type": "text"
      },
      "source": [
        "As an example, we would like to find the derivative of the function $f(x) = 2x^2$. Recalling our high school mathematics, we know that the derivative is $f'(x) = 4x$. Using this simple function as an example will help us evaluate and understand the ```autograd``` package in depth.\n",
        "\n",
        "Firstly let us assign an initial value of ```X```.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ADMGsPQCEBrv",
        "colab_type": "code",
        "outputId": "4b40f770-4ce8-4b40-d922-becc4ed8c8af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "X = nd.array([[1, 2], [3, 4]])\n",
        "X"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\n",
              "[[1. 2.]\n",
              " [3. 4.]]\n",
              "<NDArray 2x2 @cpu(0)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9s8oW0OZN2Lh",
        "colab_type": "text"
      },
      "source": [
        "Before we calculate the gradient of $f(x)$, we need to specify the storage by invoking the ```.attach_grad()``` method on the NDArray ```X```."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWlpgZLMEGPb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X.attach_grad()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHby6AFDOVfN",
        "colab_type": "text"
      },
      "source": [
        "We can check the gradient of each value in ```X``` by accessing the ```.grad``` attribute. Since we have not actually calculated anything, we expect to see zeros."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j9MPKtOtEIFP",
        "colab_type": "code",
        "outputId": "49179e4a-892b-4874-a8c8-9da2eed5fee0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "X.grad"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\n",
              "[[0. 0.]\n",
              " [0. 0.]]\n",
              "<NDArray 2x2 @cpu(0)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-39b3ZoEOmrY",
        "colab_type": "text"
      },
      "source": [
        "Now we are going to define the function $y = f(x)$. To let MXNet store $Y$, so that we can compute gradients later, we need to put the definition inside an ```autograd.record()``` scope."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tEFsPXo1EI5V",
        "colab_type": "code",
        "outputId": "4cca937f-48a1-414f-d401-80b45ddedf7a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "with autograd.record():\n",
        "    Y = 2 * X * X\n",
        "Y"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\n",
              "[[ 2.  8.]\n",
              " [18. 32.]]\n",
              "<NDArray 2x2 @cpu(0)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQ1LwEhTP4gM",
        "colab_type": "text"
      },
      "source": [
        "Outputing ```Y``` we can see that the values in ```X``` are plugged into $f(x)$. We can manually evaluate this to understand what has happened.\n",
        "\n",
        "1. $f(1) = 2 \\times 1^{2} = 2$\n",
        "2. $f(2) = 2 \\times 2^{2} = 2 \\times 4 = 8$\n",
        "3. $f(3) = 2 \\times 3^{2} = 2 \\times 9 = 18$\n",
        "4. $f(4) = 2 \\times 4^{2} = 2 \\times 16 = 32$\n",
        "\n",
        "We can see that ```Y``` now stores the value results of $f(x)$ over the values in ```X```.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WveSrHdQQmD3",
        "colab_type": "text"
      },
      "source": [
        "To calculate the gradient of $f(x)$, in other words, calculating $f'(x)$, we need to call the ```.backward()``` method on ```Y```, which invoke back propagation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FXGORqhWEYpp",
        "colab_type": "code",
        "outputId": "aa9fda7d-2182-483e-f16e-bb0c6f9864bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        }
      },
      "source": [
        "print(\"Before back-prop\")\n",
        "print(X)\n",
        "print(X.grad)\n",
        "print(Y)\n",
        "Y.backward()\n",
        "print(\"After back-prop\")\n",
        "print(X)\n",
        "print(X.grad)\n",
        "print(Y)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Before back-prop\n",
            "\n",
            "[[1. 2.]\n",
            " [3. 4.]]\n",
            "<NDArray 2x2 @cpu(0)>\n",
            "\n",
            "[[0. 0.]\n",
            " [0. 0.]]\n",
            "<NDArray 2x2 @cpu(0)>\n",
            "\n",
            "[[ 2.  8.]\n",
            " [18. 32.]]\n",
            "<NDArray 2x2 @cpu(0)>\n",
            "After back-prop\n",
            "\n",
            "[[1. 2.]\n",
            " [3. 4.]]\n",
            "<NDArray 2x2 @cpu(0)>\n",
            "\n",
            "[[ 4.  8.]\n",
            " [12. 16.]]\n",
            "<NDArray 2x2 @cpu(0)>\n",
            "\n",
            "[[ 2.  8.]\n",
            " [18. 32.]]\n",
            "<NDArray 2x2 @cpu(0)>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8sQSu7BsRiCn",
        "colab_type": "text"
      },
      "source": [
        "We can see that before calling the ```.backward()``` method on ```Y```, the values of ```X.grad``` are zeros. After invoking back propagation, ```X.grad``` gives us the gradient at each $x$ value. \n",
        "\n",
        "We can manually check this to confirm the values are as we expected.\n",
        "\n",
        "$f(x) = 2x^{2}$\n",
        "\n",
        "$f'(x) = 4x$\n",
        "\n",
        "1. $f'(1) = 4 \\times 1 = 4$\n",
        "2. $f'(2) = 4 \\times 2 = 8$\n",
        "3. $f'(3) = 4 \\times 3 = 12$\n",
        "4. $f'(4) = 4 \\times 4 = 16$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Prt2WUPvceEb",
        "colab_type": "text"
      },
      "source": [
        "### Using Python control flows\n",
        "\n",
        "Sometimes we want to write dynamic programs where the execution depends on some real-time values. MXNet will record the execution trace and compute the gradient as well.\n",
        "\n",
        "Consider the following function f(a): it doubles the inputs until its norm reaches 1000. Then it selects one element depending on the sum of its elements.\n",
        "\n",
        "We will ask it to output ```b``` during the calculation and tell us when its norm is greater than 1000."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pvgV19nXS0fG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def f(a):\n",
        "    b = a * 2\n",
        "    while b.norm().asscalar() < 1000:\n",
        "        b = b * 2\n",
        "        print(b)\n",
        "        print(b.sum().asscalar())\n",
        "        print(b.norm().asscalar())\n",
        "    \n",
        "    print(\"Done\")\n",
        "    if b.sum().asscalar() >= 0:\n",
        "        c = b[0]\n",
        "    else:\n",
        "        c = b[1]\n",
        "    return c"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTuG_LICia6R",
        "colab_type": "text"
      },
      "source": [
        "We record the trace and feed in a random value. We will let the range be between -1 and 1 so the simulation can produce a sum of ```b``` of less than 0 while the norm (```b.norm()``` is greater than 1000."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wV5-w1XsS9Cr",
        "colab_type": "code",
        "outputId": "6e888059-eecf-4f4a-a1da-099ad5ddcb87",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "a = nd.random.uniform(-1, 1, shape=2)\n",
        "print(a)\n",
        "a.attach_grad()\n",
        "with autograd.record():\n",
        "    c = f(a)\n",
        "c.backward()\n",
        "c"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[0.09762704 0.18568921]\n",
            "<NDArray 2 @cpu(0)>\n",
            "\n",
            "[0.39050817 0.74275684]\n",
            "<NDArray 2 @cpu(0)>\n",
            "1.133265\n",
            "0.8391569\n",
            "\n",
            "[0.78101635 1.4855137 ]\n",
            "<NDArray 2 @cpu(0)>\n",
            "2.26653\n",
            "1.6783139\n",
            "\n",
            "[1.5620327 2.9710274]\n",
            "<NDArray 2 @cpu(0)>\n",
            "4.53306\n",
            "3.3566277\n",
            "\n",
            "[3.1240654 5.9420547]\n",
            "<NDArray 2 @cpu(0)>\n",
            "9.06612\n",
            "6.7132554\n",
            "\n",
            "[ 6.248131  11.8841095]\n",
            "<NDArray 2 @cpu(0)>\n",
            "18.13224\n",
            "13.426511\n",
            "\n",
            "[12.496262 23.768219]\n",
            "<NDArray 2 @cpu(0)>\n",
            "36.26448\n",
            "26.853022\n",
            "\n",
            "[24.992523 47.536438]\n",
            "<NDArray 2 @cpu(0)>\n",
            "72.52896\n",
            "53.706043\n",
            "\n",
            "[49.985046 95.072876]\n",
            "<NDArray 2 @cpu(0)>\n",
            "145.05792\n",
            "107.41209\n",
            "\n",
            "[ 99.97009 190.14575]\n",
            "<NDArray 2 @cpu(0)>\n",
            "290.11584\n",
            "214.82417\n",
            "\n",
            "[199.94019 380.2915 ]\n",
            "<NDArray 2 @cpu(0)>\n",
            "580.2317\n",
            "429.64835\n",
            "\n",
            "[399.88037 760.583  ]\n",
            "<NDArray 2 @cpu(0)>\n",
            "1160.4634\n",
            "859.2967\n",
            "\n",
            "[ 799.76074 1521.166  ]\n",
            "<NDArray 2 @cpu(0)>\n",
            "2320.9268\n",
            "1718.5934\n",
            "Done\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\n",
              "[799.76074]\n",
              "<NDArray 1 @cpu(0)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14TCaUs0i_Kg",
        "colab_type": "text"
      },
      "source": [
        "We know that ```b``` is a linear function of ```a```, and ```c``` is chosen from ```b```. Then the gradient with respect to a be will be either ```[c/a[0], 0]``` or ```[0, c/a[1]]```, depending on which element from b we picked. Let’s find the results:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0vRMlhtCS-sE",
        "colab_type": "code",
        "outputId": "d373b5c6-fd8b-42ba-f824-817a8813e200",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "[a.grad, c/a]"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\n",
              " [8192.    0.]\n",
              " <NDArray 2 @cpu(0)>, \n",
              " [8192.     4306.9854]\n",
              " <NDArray 2 @cpu(0)>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LaKi6USOX_oK",
        "colab_type": "text"
      },
      "source": [
        "### A quick side note\n",
        "\n",
        "The ```.norm()``` method calculates, by default, the $l_2$ norm of a matrix. The resulting value is an array of a single value, i.e., a scalar, but it requires explicit conversion to a scalar with the method ```.asscalar()```.\n",
        "\n",
        "The section below is a manual calculation of the $l_2$ norm and is optional but will help us to gain a better understanding of the $l_2$ norm calculation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oiU_2MbmYs2A",
        "colab_type": "text"
      },
      "source": [
        "### The $l_2$ norm\n",
        "\n",
        "The $l_2$ norm is defined according to the following equation:\n",
        "\n",
        "$||X||_{2} = (\\sum^{n}_{i = 1} x_i^2)^{\\frac{1}{2}}$\n",
        "\n",
        "The equation may look complicate it but it is quite simple. Basically it is asking us to find the square root of the sum of all values in $X$ squared. This will make more sense when we go to calculate it by manually.\n",
        "\n",
        "Let's define a matrix, $X$, and calculate its norm.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F3ueskGxYq8o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = nd.array([[2, 4], [7, 5]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0nK5LW-ZpBH",
        "colab_type": "code",
        "outputId": "33714925-bdac-4e98-d6bb-26a283f17b83",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "X.norm()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\n",
              "[9.695359]\n",
              "<NDArray 1 @cpu(0)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OoHLNlXYZ1W2",
        "colab_type": "text"
      },
      "source": [
        "Now let us manually work out the norm according the equation for $l_2$ norm above.\n",
        "\n",
        "1. when $i = 1$, $x = 2$, i.e., ```X[0,0]```, hence the $x^2$ equals $4$.\n",
        "2. when $i = 2$, $x = 3$, i.e., ```X[0,1]```, hence the $x^2$ equals $16$.\n",
        "3. when $i = 3$, $x = 7$, i.e., ```X[1,0]```, hence the $x^2$ equals $49$.\n",
        "4. when $i = 4$, $x = 5$, i.e., ```X[1,1]```, hence the $x^2$ equals $25$.\n",
        "5. the sum of those values is $4 + 16 + 49 + 25 = 94$\n",
        "6. the square root of the sum is $\\sqrt{94} \\approx 9.695359$, which is the same as the value calculated by the ```.norm()``` method.\n",
        "\n",
        "Putting it all together:\n",
        "\n",
        "$||X||_2 = \\sqrt{2^2 + 4^2 + 7^2 + 5^2} = \\sqrt{94} \\approx 9.695359$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTnKVrRujOvv",
        "colab_type": "text"
      },
      "source": [
        "### The sum\n",
        "\n",
        "The method ```.sum()``` simplies add all the values together. Consider the same matrix $X$, the sum of $X$, e.g., ```X.sum()``` equals 18.\n",
        "\n",
        "$sum(X) = 2 + 4 + 7 + 5 = 18$\n",
        "\n",
        "We can check by calling the ```.sum()``` method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_o-rCsRfcrUF",
        "colab_type": "code",
        "outputId": "26dd3bd3-4fd4-44bc-bf2b-5c3b99f7c8c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "X.sum()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\n",
              "[18.]\n",
              "<NDArray 1 @cpu(0)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3D6F5Mr806L",
        "colab_type": "text"
      },
      "source": [
        "### More fun with ```autograd```\n",
        "\n",
        "Now that we have an understanding of what ```autograd``` is and how it works, let's muck around with it a little by using it to plot a graph and its gradient function. We will use ```matplotlib``` to plot the two graphs. If you are unfamiliar with ```matplotlib```, please check out my channel for the tutorials."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80S7UHvp9POR",
        "colab_type": "text"
      },
      "source": [
        "#### Defining a function\n",
        "\n",
        "Let's define a simple quadratic function, $f(x) = x^{2}. $"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "deFptHKcjoHv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def f(x):\n",
        "    y = x ** 2\n",
        "    return y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TW93zezw9hYo",
        "colab_type": "text"
      },
      "source": [
        "Let's create some $x$ values and store them in an ```NDArray``` and call the ```.attach_grad()``` method to store the gradients at the $x$ values when we perform back propagation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hx14HH-6jNDr",
        "colab_type": "code",
        "outputId": "10d02a98-c5fc-42aa-d02f-13a35bcc567f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "x = nd.array([i for i in range(-10, 11)])\n",
        "x.attach_grad()\n",
        "print(\"x\", x)\n",
        "print(\"x.grad\", x.grad)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x \n",
            "[-10.  -9.  -8.  -7.  -6.  -5.  -4.  -3.  -2.  -1.   0.   1.   2.   3.\n",
            "   4.   5.   6.   7.   8.   9.  10.]\n",
            "<NDArray 21 @cpu(0)>\n",
            "x.grad \n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "<NDArray 21 @cpu(0)>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhpFwqyf950d",
        "colab_type": "text"
      },
      "source": [
        "Let's calculate $y$ by calling the ```f(x)``` function within the ```autograd.record()``` scope."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xgZqJH4ljyJ8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "e84d18da-bd7d-481a-c32a-2886d1231341"
      },
      "source": [
        "with autograd.record():\n",
        "    y = f(x)\n",
        "print(\"y\", y)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "y \n",
            "[100.  81.  64.  49.  36.  25.  16.   9.   4.   1.   0.   1.   4.   9.\n",
            "  16.  25.  36.  49.  64.  81. 100.]\n",
            "<NDArray 21 @cpu(0)>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sdk0E33A-FYA",
        "colab_type": "text"
      },
      "source": [
        "We will now perform back propagation by calling ```.backward()```. We can store the gradients in a separate variable, ```grad```.\n",
        "\n",
        "Manually deriving $y = x^{2}$, we get $\\frac{dy}{dx} = 2x$. So if $x = -10$ the gradient should be $-20$. Inspecting the array of gradient, ```grad```, we see that the first element is ```-20```."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gp4RrlyakDSq",
        "colab_type": "code",
        "outputId": "a39fa6da-6652-4969-ce00-5972b109b7af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "y.backward()\n",
        "grad = x.grad\n",
        "print(\"grad\", grad)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "grad \n",
            "[-20. -18. -16. -14. -12. -10.  -8.  -6.  -4.  -2.   0.   2.   4.   6.\n",
            "   8.  10.  12.  14.  16.  18.  20.]\n",
            "<NDArray 21 @cpu(0)>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dv_gxGXw-1fD",
        "colab_type": "text"
      },
      "source": [
        "We'll import ```numpy``` and ```matplotlib```, and create three arrays of x values, y values and plot titles for use in ```zip()``` function for parallel iteration. We'll create a subplot and graph the two functions as $f(x)$ and $f'(x)$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2-AiR3jtjkzk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "outputId": "9b4dde0f-91c3-4ab7-a943-62e443382c55"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "x_vals = np.array([x.asnumpy(), x.asnumpy()])\n",
        "y_vals = np.array([y.asnumpy(), grad.asnumpy()])\n",
        "titles = np.array([\"f(x)\", \"f`(x)\"])\n",
        "\n",
        "fig, ax = plt.subplots(1, 2, figsize=(8, 5))\n",
        "for ax_i, x_i, y_i, t in zip(ax, x_vals, y_vals,titles):\n",
        "    ax_i.plot(x_i, y_i)\n",
        "    ax_i.set_title(t)\n",
        "    ax_i.set_xlabel(\"x\")"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeYAAAFNCAYAAADVSMziAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 576x360 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D8eukogZrhvN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}